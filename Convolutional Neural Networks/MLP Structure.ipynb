{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18d36f3e-08da-487b-82aa-a80c8a70955d",
   "metadata": {},
   "source": [
    "The loss function \n",
    " - quantifies how far we are from the ideal state where the network does not make any mistakes and has perfect confidence in its answers.\n",
    " - for image classification, the most common loss function is Categorical Cross-Entropy (CCE) loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db62ec-94c8-4428-95db-27db73486fb3",
   "metadata": {},
   "source": [
    "Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489734f4-05b4-48a9-b3bc-6fb02c070e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "## T.Compose creates a pipeline where the provided\n",
    "## transformations are run in sequence\n",
    "transforms = T.Compose(\n",
    "    [\n",
    "\n",
    "        # This transforms takes a np.array or a PIL image of integers\n",
    "        # in the range 0-255 and transforms it to a float tensor in the\n",
    "        # range 0.0 - 1.0\n",
    "        T.ToTensor(),\n",
    "\n",
    "        # This then renormalizes the tensor to be between -1.0 and 1.0,\n",
    "        # which is a better range for modern activation functions like\n",
    "        # Relu\n",
    "        T.Normalize((0.5), (0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"data\", train=True, download=True, transform=transforms\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\", train=False, download=True, transform=transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dbde6d-9583-4e87-bfb5-4c933c4b8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to differentiate, train=True indicates training data, train=False indicates testing data\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"data\", train=True, download=True, transform=transforms\n",
    ")\n",
    "\n",
    "# batch size indicates the size of the mini-batch for stochastic gradient\n",
    "# num_worders indicates the number of processes that PyTorch should use to load the data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  dataset=train_data, \n",
    "  shuffle=True, \n",
    "  batch_size=batch_size,\n",
    "  num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621395a-d5bb-47bf-9828-209b753c5805",
   "metadata": {},
   "source": [
    "A good rule of thumb: use a number of workers equal to the number of CPUs on the current machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca844f-78e1-47f8-bd5f-5161c87d1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "n_workers = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c671c-39fc-4a29-864a-20e4e02d9366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to loop data\n",
    "for image_batch, label_batch in train_loader:\n",
    "#    ... do something ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0fc8fa-8892-4848-a444-c348faf309c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get an iterator from the dataloader\n",
    "dataiter = iter(train_loader)\n",
    "## Get the next batch\n",
    "image_batch, label_batch = dataiter.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b2366-1d77-4a84-b2f5-fb52ca9cc67f",
   "metadata": {},
   "source": [
    "splitting training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a3b2b-78e2-47eb-9e41-b24982a33ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's keep 80% of the training data for training\n",
    "train_len = int(len(trainval_data) * 0.8)\n",
    "\n",
    "## Let's use the remaining for validation\n",
    "val_len = len(trainval_data) - train_len\n",
    "\n",
    "## Perform a random split of the train dataset\n",
    "train_subset, val_subset = torch.utils.data.random_split(\n",
    "    trainval_data, [train_len, val_len]\n",
    ")\n",
    "\n",
    "## Now we can use the subsets as normal datasets\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_subset, shuffle=True, batch_size=batch_size, num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_subset, shuffle=False, batch_size=batch_size, num_workers=num_workers\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
