{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f4cbf1a-b4df-473f-8936-be478e6e962c",
   "metadata": {},
   "source": [
    "CNNs vs. MLP\n",
    "CNN does don't need to flatten the image, can utilize the spatial structure of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0412a2-732d-4025-9401-e9d62d251d22",
   "metadata": {},
   "source": [
    "Characterized by Locally-Connected Layers\n",
    "- layers where neurons are connected to only a limited numbers of input pixels\n",
    "- neurons share their weights, which drastically reduces the number of parameters\n",
    "- capable of extracting spatial and color patterns that characterize different objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6cff55-8b65-4835-b07a-3b4b2957f01a",
   "metadata": {},
   "source": [
    "Filter/kernel\n",
    "- \"extract\" the features of an object (for example, edges).\n",
    "- By using multiple different filters the network can learn to recognize complex shapes and objects.\n",
    "- relies on centering a pixel and looking at its surrounding neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e944f6b-4410-4b38-8ba1-ccdfc6e688e8",
   "metadata": {},
   "source": [
    "Frequency in Images\n",
    "- means rate of change\n",
    "- high-frequency image: where the intensity changes a lot. And the level of brightness changes quickly from one pixel to the next.\n",
    "- low-frequency image may be one that is relatively uniform in brightness or changes very slowly\n",
    "\n",
    "Padding - The image is padded with a border of 0's, black pixels.\n",
    "\n",
    "Cropping - Any pixel in the output image which would require values from beyond the edge is skipped. This method can result in the output image being smaller then the input image, with the edges having been cropped.\n",
    "\n",
    "Extension - The nearest border pixels are conceptually extended as far as necessary to provide values for the convolution. Corner pixels are extended in 90Â° wedges. Other edge pixels are extended in lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78faddd-2e8a-4714-bb8b-8f0f349f2699",
   "metadata": {},
   "source": [
    "Pooling\n",
    "- compresses information from a layer by summarizing areas of the feature maps produced in that layer.\n",
    "- compute a summary statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07ff43-1af6-45c8-bbc8-8bbe7b04e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e861ebe-e837-439a-a062-d69e86bb6c1c",
   "metadata": {},
   "source": [
    "in_channels - The number of input feature maps. \n",
    "    - If this is the first layer, this is equivalent to the number of channels in the input image, i.e., 1 for grayscale images, or 3 for color images (RGB). \n",
    "    - Otherwise, it is equal to the output channels of the previous convolutional layer.\n",
    "\n",
    "out_channels - The number of output feature maps (channels), i.e. the number of filtered \"images\" that will be produced by the layer. \n",
    "    - This corresponds to the unique convolutional kernels that will be applied to an input, because each kernel produces one feature map/channel. \n",
    "    - Determining this number is an important decision to make when designing CNNs, just like deciding on the number of neurons is an important decision for an MLP.\n",
    "    \n",
    "kernel_size - Number specifying both the height and width of the (square) convolutional kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3595f2d-587e-4051-9fca-b6f8c9138b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591792f1-cd64-47f9-b278-d69a44a74955",
   "metadata": {},
   "source": [
    "We need activation layer and a 2D dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a3f68-22db-42e2-bfd2-cc7700150abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "dropout1 = nn.Dropout2d(p=0.2)\n",
    "relu1 = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379e8e4-e8e8-4499-9e06-c905bffa0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = relu1(dropout1(conv1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac8bd0-9a26-4a6e-9ad3-1a7ffcd9ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Alternati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25926a68-51a1-4035-b4eb-6533a3a2b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_block = nn.Sequential(\n",
    "  nn.Conv2d(in_channels, out_channels, kernel_size),\n",
    "  nn.ReLU(),\n",
    "  nn.Dropout2d(p=0.2)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
