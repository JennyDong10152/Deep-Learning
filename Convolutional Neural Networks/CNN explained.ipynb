{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f4cbf1a-b4df-473f-8936-be478e6e962c",
   "metadata": {},
   "source": [
    "CNNs vs. MLP\n",
    "CNN does don't need to flatten the image, can utilize the spatial structure of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782d52b-678c-47fd-9ab6-d94c0df34cef",
   "metadata": {},
   "source": [
    "The following explains the backbone structure of a classical CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0412a2-732d-4025-9401-e9d62d251d22",
   "metadata": {},
   "source": [
    "Characterized by Locally-Connected Layers\n",
    "- layers where neurons are connected to only a limited numbers of input pixels\n",
    "- neurons share their weights, which drastically reduces the number of parameters\n",
    "- capable of extracting spatial and color patterns that characterize different objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6cff55-8b65-4835-b07a-3b4b2957f01a",
   "metadata": {},
   "source": [
    "Filter/kernel\n",
    "- \"extract\" the features of an object (for example, edges).\n",
    "- By using multiple different filters the network can learn to recognize complex shapes and objects.\n",
    "- relies on centering a pixel and looking at its surrounding neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e944f6b-4410-4b38-8ba1-ccdfc6e688e8",
   "metadata": {},
   "source": [
    "Frequency in Images\n",
    "- means rate of change\n",
    "- high-frequency image: where the intensity changes a lot. And the level of brightness changes quickly from one pixel to the next.\n",
    "- low-frequency image may be one that is relatively uniform in brightness or changes very slowly\n",
    "\n",
    "Padding - The image is padded with a border of 0's, black pixels.\n",
    "\n",
    "Stride: Amount by which a filter slides over an image.\n",
    "\n",
    "Cropping - Any pixel in the output image which would require values from beyond the edge is skipped. This method can result in the output image being smaller then the input image, with the edges having been cropped.\n",
    "\n",
    "Extension - The nearest border pixels are conceptually extended as far as necessary to provide values for the convolution. Corner pixels are extended in 90Â° wedges. Other edge pixels are extended in lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78faddd-2e8a-4714-bb8b-8f0f349f2699",
   "metadata": {},
   "source": [
    "Pooling\n",
    "- compresses information from a layer by summarizing areas of the feature maps produced in that layer.\n",
    "- compute a summary statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b05a2-6a5b-4f8b-823e-7a4c18fc0893",
   "metadata": {},
   "source": [
    "Image -> conv1 -> maxpool -> conv2 -> maxpool -> conv3 -> maxpool ...\n",
    "The size gets smaller as you go deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07ff43-1af6-45c8-bbc8-8bbe7b04e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e861ebe-e837-439a-a062-d69e86bb6c1c",
   "metadata": {},
   "source": [
    "in_channels - The number of input feature maps. \n",
    "    - If this is the first layer, this is equivalent to the number of channels in the input image, i.e., 1 for grayscale images, or 3 for color images (RGB). \n",
    "    - Otherwise, it is equal to the output channels of the previous convolutional layer.\n",
    "\n",
    "out_channels - The number of output feature maps (channels), i.e. the number of filtered \"images\" that will be produced by the layer. \n",
    "    - This corresponds to the unique convolutional kernels that will be applied to an input, because each kernel produces one feature map/channel. \n",
    "    - Determining this number is an important decision to make when designing CNNs, just like deciding on the number of neurons is an important decision for an MLP.\n",
    "    \n",
    "kernel_size - Number specifying both the height and width of the (square) convolutional kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3595f2d-587e-4051-9fca-b6f8c9138b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591792f1-cd64-47f9-b278-d69a44a74955",
   "metadata": {},
   "source": [
    "We need activation layer and a 2D dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a3f68-22db-42e2-bfd2-cc7700150abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "dropout1 = nn.Dropout2d(p=0.2)\n",
    "relu1 = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379e8e4-e8e8-4499-9e06-c905bffa0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = relu1(dropout1(conv1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11835dbf-151d-4286-be48-115993b62dbc",
   "metadata": {},
   "source": [
    "Alternatively, we can use nn.Sequential and stack together the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25926a68-51a1-4035-b4eb-6533a3a2b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_block = nn.Sequential(\n",
    "  nn.Conv2d(in_channels, out_channels, kernel_size),\n",
    "  nn.ReLU(),\n",
    "  nn.Dropout2d(p=0.2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af4ece-0a89-46f8-893c-f2fab63ed581",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = conv_block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee69849-c8d8-46d8-98de-90fcb4eca539",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a01b8-f0fa-4453-a1e8-ef643bf89cc8",
   "metadata": {},
   "source": [
    "padding=\"same\": automatically compute the amount necessary padding so that output size = input size\n",
    "padding=\"valid\": padding = 0\n",
    "padding_mode=\"reflect\": the padding pixels are filled with a copy of the values in the input image taken in opposite order, in a mirroring fashion.\n",
    "padding_mode=\"replicate\": the padding pixels are filled with value of closest pixel in input image\n",
    "padding_mode=\"circular\": it is like the reflection mode, but the image is first flipped horizontally and vertically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176687b-0327-496c-8cdf-9a77b515b03a",
   "metadata": {},
   "source": [
    "kernel_size - The size of the max pooling window. The layer will roll a window of this size over the input feature map and select the maximum value for each window.\n",
    "stride - The stride for the operation. By default the stride is of the same size as the kernel (i.e., kernel_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd11431-da2e-43f3-ba8e-f422dd446ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "nn.MaxPool2d(kernel_size, stride)\n",
    "\n",
    "pooling = nn.AvgPool2d(window_size, stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e89857-821f-4b22-a61c-bf15a0f4141f",
   "metadata": {},
   "source": [
    "The following explains the Head of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f564eb04-885b-4cc9-9e39-204c04a24568",
   "metadata": {},
   "source": [
    "Flattening layer > after backbone, before head. Process the output from backbone into a 1d vector\n",
    "feature vector/embedding: for each feature map the rows are stacked together in a 1d vector, then all the 1d vectors are stacked together to form a long 1d vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a7041-1fcf-403e-8c96-f0d9c60fdbdb",
   "metadata": {},
   "source": [
    "Head \n",
    "- a normal MLP that takes as input the feature vector and has the appropriate output for the task.\n",
    "- It can have one or more hidden layers, as well as other types of layers as needed (like DropOut for regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a03b5-ced1-49f2-9c43-9bfe94429f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
