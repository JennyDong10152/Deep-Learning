{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7686654-40cc-4cf4-ad28-d6db15cf1fef",
   "metadata": {},
   "source": [
    "General rule of thumb -> 80% training, 10% validating with random datapoints, and 10% testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf89f93-254b-41a3-95ec-83fff078d986",
   "metadata": {},
   "source": [
    "k-fold cross-validation -> the data is split up into some number, which we call k, equal parts. One is used as the validation set, one is used as the test set, and the remaining parts are used as the training set. We then cycle through all combinations of the data until all parts have had a chance to be the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a6b42-9195-4dcf-a6ed-748c31b5a6dc",
   "metadata": {},
   "source": [
    "Underfitting -> error due to bias. Our training data may be biased and this bias may be incorporated into the model in a way that oversimplifies it\n",
    "Overfitting -> error due to variance. This means that there are random or irrelevant differences among the data points in our training data and we have fit the model so closely to these irrelevant differences that it performs poorly when we try to use it with our testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc724ea-fff5-4b34-b9c3-5e6c4e8b1730",
   "metadata": {},
   "source": [
    "Early stopping -> we do gradient descent until the testing error stops decreasing and starts to increas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7721ab-5bb0-4ca1-bfa7-87d294c6873c",
   "metadata": {},
   "source": [
    "Tensorboard for visualizing training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3138066-dedc-4366-ad36-2f0281c0b559",
   "metadata": {},
   "source": [
    "regularization\n",
    "penalize large coefficients. They do well in error, but might cause overfitting\n",
    "L2 regularization use squares, tends not to favor sparse vectors since it tries to maintain all the weights homogeneously small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c7030-9ca4-4a38-8a70-75d71c1a8465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_regularization(weights, lamb):\n",
    "    absolute_weights = np.abs(weights)\n",
    "    return lamb * np.sum(absolute_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd9f7b-5199-4792-bad3-37b181d38e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularization(weights, lamb):\n",
    "    squared_weights = np.dot(weights, weights.T)\n",
    "    return lamb * squared_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36504272-c5fd-41f7-b7fb-4f3676d0d8cd",
   "metadata": {},
   "source": [
    "dropout ->  we turn part of the network off and let the rest of the network train, this ensures that large weights don't dominate the network\n",
    "\n",
    "We go through the epochs and randomly turn off some of the nodes. This forces the other nodes to pick up the slack and take a larger part in the training.\n",
    "To drop nodes, we give the algorithm a parameter that indicates the probability that each node will get dropped during each epoch. For example, if we set this parameter to 0.2, this means that during each epoch, each node has a 20% probability of being turned off.\n",
    "Note that some nodes may get turned off more than others and some may never get turned off. This is OK since we're doing it over and over; on average, each node will get approximately the same treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657d819-1100-4be6-9168-9348fe8e22b0",
   "metadata": {},
   "source": [
    "Vanishing Gradient -> as error backpropogates, the error signal gets weaker\n",
    "saturating activation functions -- those that have a bounded range might cause vanishing gradient problem\n",
    "weight drop to 0 or high but stalled training loss\n",
    "\n",
    "Exploding gradient\n",
    "really large neuron weight or unstable loss\n",
    "\n",
    "solution: change activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9de72d",
   "metadata": {},
   "source": [
    "Learning rate:\n",
    "if too big, my might overshoot the minimum\n",
    "if too small, too slow\n",
    "Ideally we want larger learning rate in the beginning then smaller as we approach minimum "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca78aba8",
   "metadata": {},
   "source": [
    "Batch Gradient Descent\n",
    "In order to decrease error, we take a bunch of steps following the negative of the gradient, which is the error function.\n",
    "\n",
    "Each step is called an epoch.\n",
    "In each epoch, we take our input (all of our data) and run it through the entire neural network.\n",
    "Then we find our predictions and calculate the error (how far the predictions are from the actual labels).\n",
    "Finally, we back-propagate this error in order to update the weights in the neural network. This will give us a better boundary for predicting our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d57c5e-fc15-4aae-a2bf-cf5af459c014",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent\n",
    "We take small subsets of the data and run them through the neural network, calculating the gradient of the error function based on these points and moving one step in that direction.\n",
    "\n",
    "Split the data into several batches.\n",
    "Take the points in the first batch and run them through the neural network.\n",
    "Calculate the error and its gradient.\n",
    "Back-propagate to update the weights (which will define a better boundary region).\n",
    "Repeat the above steps for the rest of the batches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
