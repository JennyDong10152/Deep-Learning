{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7686654-40cc-4cf4-ad28-d6db15cf1fef",
   "metadata": {},
   "source": [
    "General rule of thumb -> 80% training, 10% validating with random datapoints, and 10% testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf89f93-254b-41a3-95ec-83fff078d986",
   "metadata": {},
   "source": [
    "k-fold cross-validation -> the data is split up into some number, which we call k, equal parts. One is used as the validation set, one is used as the test set, and the remaining parts are used as the training set. We then cycle through all combinations of the data until all parts have had a chance to be the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a6b42-9195-4dcf-a6ed-748c31b5a6dc",
   "metadata": {},
   "source": [
    "Underfitting -> error due to bias. Our training data may be biased and this bias may be incorporated into the model in a way that oversimplifies it\n",
    "Overfitting -> error due to variance. This means that there are random or irrelevant differences among the data points in our training data and we have fit the model so closely to these irrelevant differences that it performs poorly when we try to use it with our testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc724ea-fff5-4b34-b9c3-5e6c4e8b1730",
   "metadata": {},
   "source": [
    "Early stopping -> we do gradient descent until the testing error stops decreasing and starts to increas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7721ab-5bb0-4ca1-bfa7-87d294c6873c",
   "metadata": {},
   "source": [
    "Tensorboard for visualizing training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3138066-dedc-4366-ad36-2f0281c0b559",
   "metadata": {},
   "source": [
    "regularization\n",
    "penalize large coefficients. They do well in error, but might cause overfitting\n",
    "L2 regularization use squares, tends not to favor sparse vectors since it tries to maintain all the weights homogeneously small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c7030-9ca4-4a38-8a70-75d71c1a8465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_regularization(weights, lamb):\n",
    "    absolute_weights = np.abs(weights)\n",
    "    return lamb * np.sum(absolute_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd9f7b-5199-4792-bad3-37b181d38e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularization(weights, lamb):\n",
    "    squared_weights = np.dot(weights, weights.T)\n",
    "    return lamb * squared_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36504272-c5fd-41f7-b7fb-4f3676d0d8cd",
   "metadata": {},
   "source": [
    "dropout ->  we turn part of the network off and let the rest of the network train, this ensures that large weights don't dominate the network\n",
    "\n",
    "We go through the epochs and randomly turn off some of the nodes. This forces the other nodes to pick up the slack and take a larger part in the training.\n",
    "To drop nodes, we give the algorithm a parameter that indicates the probability that each node will get dropped during each epoch. For example, if we set this parameter to 0.2, this means that during each epoch, each node has a 20% probability of being turned off.\n",
    "Note that some nodes may get turned off more than others and some may never get turned off. This is OK since we're doing it over and over; on average, each node will get approximately the same treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657d819-1100-4be6-9168-9348fe8e22b0",
   "metadata": {},
   "source": [
    "Vanishing Gradient -> as error backpropogates, the error signal gets weaker\n",
    "saturating activation functions -- those that have a bounded range might cause vanishing gradient problem\n",
    "weight drop to 0 or high but stalled training loss\n",
    "\n",
    "Exploding gradient\n",
    "really large neuron weight or unstable loss\n",
    "\n",
    "solution: change activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e964e0-3db6-484f-8119-f2f78b5ea4f5",
   "metadata": {},
   "source": [
    "Learning rate\n",
    "If too big, faster but might overshoot minimum\n",
    "If too small, too sl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
